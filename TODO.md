# Things to do for <name of project>

1. Get this thing to figure out non-standard sitemaps, like at https://www.google.com/slides/sitemaps.xml
2. Just normal overall code clean up and optimization. Currently, it takes around 12-15 seconds to pull, write, parse,
delete, and write the discovered URLs to file (basically, perform all the functionality) for https://www.tiktok.com/robots.txt,
which is about 16 files and 5200 URLs so it can be much faster
3. 
4. 
